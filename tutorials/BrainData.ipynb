{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fad67c83-ec19-4d1b-85fc-e8aa08d44009",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "from torchvision import models\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f92abfa-6886-4305-a05a-4b7dca56abe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from FC_NN_Architecture import *\n",
    "# Dataset Loader File\n",
    "from DF_DataLoader import initDataset, toDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dcaca772-3048-444c-a60d-1f5116883b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data visualization of NN\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "255d24f9-d98f-467f-9925-752a18e3ae83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "from matplotlib import cm\n",
    "import matplotlib\n",
    "from matplotlib.ticker import MaxNLocator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ec3de4-98c1-48d7-bedb-ab183275c452",
   "metadata": {},
   "source": [
    "# Get brain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18e276de-8028-44d7-908e-06bbee350b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_path = \"BrainData/BrainData/BLSA_1742_04_MCIAD_m41/rS_slice5.mat\"\n",
    "raw_data = scipy.io.loadmat(raw_data_path)\n",
    "raw_signals = raw_data[\"slice_oi\"]\n",
    "raw_masking_idxs = raw_signals[:,:,0] > 700\n",
    "truth_path = \"BrainData/BrainData/BLSA_1742_04_MCIAD_m41/m41_dataStruct_slice5.mat\"\n",
    "truth_data = scipy.io.loadmat(truth_path)\n",
    "slice_c1 = truth_data['slice'][\"MWF\"][0,0]\n",
    "reshaped_c1 = slice_c1[:,:, np.newaxis]\n",
    "reshaped_c1[~raw_masking_idxs] = reshaped_c1.min()\n",
    "reshaped_t21 = truth_data['slice']['T2s'][0,0][:,:, np.newaxis]\n",
    "reshaped_t21[~raw_masking_idxs] = reshaped_t21.min()\n",
    "reshaped_t22 = truth_data['slice']['T2l'][0,0][:,:, np.newaxis]\n",
    "reshaped_t22[~raw_masking_idxs] = reshaped_t22.min()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291d0570",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap, Normalize\n",
    "aic_mask = scipy.io.loadmat(\"BrainData/BrainData/BLSA_1742_04_MCIAD_m41/NESMA_NLLS_results.mat\")[\"aic_mask\"] == 1.0 #load AIC Mask\n",
    "mask = aic_mask.squeeze() \n",
    "x = np.random.randn(*mask.shape) \n",
    "masked_x = np.ma.array(x, mask=mask)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "base_cmap = plt.cm.terrain\n",
    "ax.imshow(mask, cmap=ListedColormap(['black'])) # this is the mask\n",
    "a1 = ax.imshow(masked_x, cmap=base_cmap) # plotting the masked matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a526fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_masking_idxs_2 = np.zeros_like(raw_masking_idxs)\n",
    "raw_masking_idxs_2[~aic_mask.squeeze()] = raw_masking_idxs[~aic_mask.squeeze()]\n",
    "\n",
    "raw_ND_signals = raw_signals[raw_masking_idxs]\n",
    "raw_norm = raw_ND_signals/raw_ND_signals[:,0][:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93acf1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(30,20), tight_layout = False)\n",
    "c1_plot = fig.add_subplot(1,1,1)\n",
    "c1_MWF = reshaped_c1\n",
    "c1_MWF[~raw_masking_idxs] = 0.0\n",
    "im_c1 = c1_plot.imshow(c1_MWF[75:215, 56:227], cmap='hot', vmin = 0, vmax = 0.50)\n",
    "c1_plot.set_title(\"Myelin Water Fraction, Precise Estimates\", size = 60, y = 1.02)\n",
    "# c1_plot.set_ylim(c1_plot.get_ylim()[::-1])\n",
    "cbar = fig.colorbar(im_c1, ax = c1_plot, location = 'bottom', pad = 0.05, shrink = 0.6)\n",
    "cbar.set_label(label = \"$c_1^{truth}$\", size = 60, labelpad = 10)\n",
    "cbar.ax.tick_params(size=40, labelsize=40)\n",
    "c1_plot.tick_params(axis='both', which='major', labelsize=40)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740c525f-fb82-4386-acf0-aca7c03da93a",
   "metadata": {},
   "source": [
    "# Set up NNs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24252de-2af2-4f48-8f8b-6c6a9eb1c125",
   "metadata": {},
   "source": [
    "First, the Lambda nets: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbe01c41-8eeb-4a7b-ba30-c30b9149b9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LambdaLoader(Dataset):\n",
    "    \n",
    "    def __init__(self, noisy_signals, snrs):\n",
    "        print('BEGIN MULTIPLE DECAYS!!!')\n",
    "\n",
    "        training_tensor_preproc = torch.from_numpy(noisy_signals).float()\n",
    "        # self.target_tensor_proc = torch.from_numpy(np.stack([t21_t, t22_t, c1_t], axis =1))\n",
    "        self.training_tensor_proc = training_tensor_preproc.unsqueeze(1)\n",
    "        self.num_times = len(self.training_tensor_proc[0]) #alter to match noisy signal only\n",
    "        self.snrs = snrs\n",
    "\n",
    "    def __len__(self):\n",
    "        #decay_input = ['ND', 'NB']\n",
    "        \"\"\"\n",
    "\n",
    "        Returns the number of samples in the data set\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        return len(self.training_tensor_proc)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        Returns samples at idx.\n",
    "\n",
    "        - If idx is an int, the output is a pair of 1-d Tensors: the first is\n",
    "\n",
    "        the regularized decays stacked horizontally and the second is the\n",
    "\n",
    "        corresponding target parameters.\n",
    "\n",
    "        - If idx is a list, it returns two Tensors, each with 2 dimensions.\n",
    "\n",
    "        The first index points to the particular sample and the second index\n",
    "\n",
    "        points to an intensity at some time for some regularized decay.\n",
    "\n",
    "        \"\"\"\n",
    "        #decay_input = ['ND', 'NB']\n",
    "        ND_tensor = self.training_tensor_proc[idx] \n",
    "        # target = self.target_tensor_proc[idx]\n",
    "\n",
    "        # print(multiple_decays)\n",
    "        return ND_tensor, self.snrs[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09da78d1-ec8f-4467-946e-c2af00b3da60",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb042fff-75d7-47a0-95ca-65cdc45b0fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_lambda(snr, device, data, standard_mean, standard_std):\n",
    "\n",
    "    lambda_model_path =f\"LambdaSelectionNetworks/Experimentation_DenseRician_LambdaNN_SNR_{snr}MPE_1_lr1e3_B512.pth\"\n",
    "    checkpoint = torch.load(lambda_model_path, map_location=device)\n",
    "\n",
    "    compiled_model = LambdaTraining_FC_NN_Convolutional_SELU(in_channel=1, out1 = 128, out2 = 256, k1 = 5, k2 = 3, fc1 = 512, fc2 = 64, out_dim=1)\n",
    "    compiled_model = compiled_model.to(device)\n",
    "    compiled_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    compiled_model.eval()\n",
    "    with torch.no_grad():\n",
    "        return (compiled_model(data) * standard_std) + standard_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "519ee7d5-e9d0-4d6a-8d15-511ae3974d96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEGIN MULTIPLE DECAYS!!!\n",
      "BEGIN MULTIPLE DECAYS!!!\n",
      "BEGIN MULTIPLE DECAYS!!!\n"
     ]
    }
   ],
   "source": [
    "convolutional=True\n",
    "training_path_5 = f\"LambdaData_snr100/DenseGraph_ExpectationRicianNoise_1000NR_SNR_{5.0}_TrainingData.feather\"\n",
    "training_dataset_5 = initDataset(training_path_5, set_type = \"training\", type1 = \"standardized\", convolutional = convolutional)\n",
    "\n",
    "training_path_50 = f\"LambdaData_snr100/DenseGraph_ExpectationRicianNoise_1000NR_SNR_{50.0}_TrainingData.feather\"\n",
    "training_dataset_50 = initDataset(training_path_50, set_type = \"training\", type1 = \"standardized\", convolutional = convolutional)\n",
    "\n",
    "training_path_100 = f\"LambdaData_snr100/DenseGraph_ExpectationRicianNoise_1000NR_SNR_{100.0}_TrainingData.feather\"\n",
    "training_dataset_100 = initDataset(training_path_100, set_type = \"training\", type1 = \"standardized\", convolutional = convolutional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42f1657-ef35-42d5-91ea-8c38023bc931",
   "metadata": {},
   "source": [
    "Now get the curve fits: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44834fff-e042-4edb-8a52-d33b6cc4da9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import special\n",
    "import scipy\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "def mycurvefit_l2Regularized_3param(snr, noisy_decay, lam, \\\n",
    "                                    signalType=\"biexponential\", \\\n",
    "                                    lb_T21=0.0, lb_T22=0.0, lb_c1=0.0, \\\n",
    "                                    ub_T21=np.inf, ub_T22=np.inf, ub_c1=np.inf): #c1 ub = 1.0??\n",
    "    SNR = snr\n",
    "    times = np.linspace(8.0, 256.0, 32)\n",
    "\n",
    "    D = np.array([1.0,  # T2,1\n",
    "\n",
    "                1.0,  # T2,2\n",
    "\n",
    "                100.0])  # C1\n",
    "\n",
    "    ld = lam.flatten().item()\n",
    "    d = noisy_decay.flatten()\n",
    "    assert(len(d) == 32)\n",
    "\n",
    "    \n",
    "    def signal(t_vec, p1, p2, p3):\n",
    "        #         p1 = t1\n",
    "        #         p2 = t2\n",
    "        #         p3 = c1\n",
    "        if signalType == \"biexponential\":\n",
    "            return p3*np.exp(-t_vec/p1) + (1.0-p3)*np.exp(-t_vec/p2)\n",
    "\n",
    "    def expectation_Rice(xdata,t21,t22, c1):\n",
    "        sigma=np.max(d)/SNR\n",
    "        t_vec = xdata[:len(d)]\n",
    "        ld_val = xdata[len(d)]\n",
    "        alpha=(signal(t_vec, t21, t22, c1)/(2*sigma))**2\n",
    "        Expectation = sigma*np.sqrt(np.pi/2)*((1+2*alpha)*special.ive(0, alpha) + 2*alpha*special.ive(1,alpha))\n",
    "        params = np.array([t21, t22, c1], dtype=np.float64)\n",
    "        penalty_vec = ld_val*np.multiply(D,params)\n",
    "        return np.concatenate((Expectation, penalty_vec))                              \n",
    "\n",
    "\n",
    "    t_dim = times.ndim\n",
    "    indep_var = np.concatenate((times, np.array(ld,ndmin=t_dim)))\n",
    "    d_dim = d.ndim\n",
    "    depen_var = np.concatenate((d, np.array(0.0, ndmin=d_dim), np.array(0.0,ndmin=d_dim), np.array(0.0, ndmin=d_dim)))     \n",
    "    try:\n",
    "        opt_val = curve_fit(expectation_Rice, indep_var, depen_var,  # curve, xdata, ydata\n",
    "                            p0=np.array([(1.0+50.0)/2,(40.0+225.0)/2,(0.0+0.60)/2]),  # initial guess\n",
    "                            bounds=([lb_T21, lb_T22, lb_c1], [ub_T21, ub_T22, ub_c1]),\n",
    "                            method=\"trf\",\n",
    "                            max_nfev=1000)\n",
    "        #print('!!!!!!!!!!', opt_val)\n",
    "    except RuntimeError:\n",
    "        opt_val = (np.asarray([66.08067015, 66.44472936, 45.5891436 ]), np.asarray([[ 6.51065099e+03, -6.53878183e+03,  1.61616729e+06], [-6.53878183e+03,  6.85069747e+03, -1.65784453e+06], [ 1.61616729e+06, -1.65784453e+06,  4.05431645e+08]]))\n",
    "        print(\"maximum number of function evaluations == exceeded\")\n",
    "    T21_ld, T22_ld, c1_ret = opt_val[0]\n",
    "# Enforces T21 <= T22\n",
    "# T21_ld = np.where(T21_ld > T22_ld, T22_ld, T21_ld)\n",
    "    if T21_ld.size == 1:\n",
    "        T21_ld = T21_ld.item()\n",
    "        T22_ld = T22_ld.item()\n",
    "        c1_ret = c1_ret.item()\n",
    "\n",
    "    if T21_ld > T22_ld:\n",
    "        T21_ld_new = T22_ld\n",
    "        T22_ld = T21_ld\n",
    "        T21_ld = T21_ld_new\n",
    "        c1_ret = 1.0 - c1_ret\n",
    "        assert (T21_ld != T22_ld)\n",
    "    return d, ld, c1_ret, T21_ld, T22_ld "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962e10cb-656b-4b9e-83ba-7502d7add817",
   "metadata": {},
   "source": [
    "Now fit the curves and estimate SNR: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9bb49a-f35f-46fd-b621-880f2f714fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = \"cuda:0\"\n",
    "with torch.no_grad():\n",
    "    # display(training_preds_t_Regs.shape)\n",
    "    curr_pos = 0\n",
    "    end_pos = 0\n",
    "    path_NLLS = \"Brain_ILR/BrainData/BrainData/BLSA_1742_04_MCIAD_m41/NLLS_3PE_with_offset_Rician.feather\"\n",
    "    df = pd.read_feather(path_NLLS)\n",
    "    df[\"Index\"] = df[\"Index\"].astype(int)\n",
    "    df.set_index(\"Index\", inplace = True, drop = True)\n",
    "    df.sort_index(inplace = True)\n",
    "    nd_off_array = np.array(df[[\"ND\",\"Offset\"]])\n",
    "    ND = nd_off_array[:,0]\n",
    "    ND = np.stack(ND)\n",
    "    ND = np.float64(ND)\n",
    "    offsets = np.float64(nd_off_array[:,1])\n",
    "    const_value = np.pi/2\n",
    "    snr_pred = ND[:,0]/(offsets/np.sqrt(const_value))\n",
    "    snr_pred = torch.from_numpy(snr_pred).float()\n",
    "    brain_NLLS_SNRs = torch.empty((288,288,1))\n",
    "    brain_NLLS_SNRs[raw_masking_idxs] = snr_pred.unsqueeze(1)\n",
    "    brain_NLLS_SNRs[~raw_masking_idxs] = 0.0\n",
    "    brainData = LambdaLoader(raw_norm, brain_NLLS_SNRs[raw_masking_idxs])\n",
    "    # brainLoader = DataLoader(brainData, batch_size = 1, shuffle = False, num_workers= 64, pin_memory = True, persistent_workers=True) #Need to do individual SNRs --> batchsize = 1\n",
    "    brainLoader = DataLoader(brainData, batch_size = 1, shuffle = False)\n",
    "    training_preds_t_Regs = torch.empty(brainLoader.dataset.training_tensor_proc.shape[0], 1)\n",
    "    lis = []\n",
    "    for idx, (noisy_decay, snr) in enumerate(tqdm(brainLoader, unit = \"batch\")):\n",
    "        batch_size = noisy_decay.size(0)\n",
    "        end_pos += batch_size\n",
    "        # assert (torch.all(noisy_decay == (brainLoader.dataset.training_tensor_proc)[curr_pos : end_pos]))\n",
    "        noisy_decay = noisy_decay.to(device)\n",
    "        if snr < 25:\n",
    "            predictions = fetch_lambda(5.0, device, noisy_decay, training_dataset_5.mean2, training_dataset_5.stdev)\n",
    "            NS, ld, c1, t21, t22 = mycurvefit_l2Regularized_3param(5.0, noisy_decay.detach().cpu().numpy(), predictions.detach().cpu().numpy())\n",
    "\n",
    "        elif snr < 75:\n",
    "            predictions = fetch_lambda(50.0, device, noisy_decay, training_dataset_50.mean2, training_dataset_50.stdev)\n",
    "            NS, ld, c1, t21, t22 = mycurvefit_l2Regularized_3param(50.0, noisy_decay.detach().cpu().numpy(), predictions.detach().cpu().numpy())\n",
    "\n",
    "        else:\n",
    "            predictions = fetch_lambda(100.0, device, noisy_decay, training_dataset_100.mean2, training_dataset_100.stdev)\n",
    "            NS, ld, c1, t21, t22 = mycurvefit_l2Regularized_3param(100.0, noisy_decay.detach().cpu().numpy(), predictions.detach().cpu().numpy())\n",
    "\n",
    "        \n",
    "        \n",
    "        lis.append(np.concatenate([np.array([c1, t21, t22, ld]), NS], dtype=np.float64))\n",
    "\n",
    "\n",
    "        # display(predictions.shape)\n",
    "        training_preds_t_Regs[curr_pos : end_pos] = predictions\n",
    "        curr_pos += batch_size\n",
    "    \n",
    "    T2_ld = np.stack([row for row in lis], axis = 0)\n",
    "    df_2 = pd.DataFrame(index = range(T2_ld.shape[0]), columns = [\"c1_ld\",\"t21_ld\", \"t22_ld\",\"lambda\", \"ND\"])\n",
    "    df_2[[\"c1_ld\", \"t21_ld\", \"t22_ld\", \"lambda\"]] = T2_ld[:,:-32]\n",
    "    df_2[\"ND\"] = [T2_ld[i,-32:] for i in range(T2_ld.shape[0])]\n",
    "    print(\"DATAFRAME: \", df_2.shape)\n",
    "    df_2.to_feather(\"brainRegSignals_RicianNoise_BinnedNNs.feather\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd97d87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2cdb0197-3816-4ce3-b077-d9f45bb443d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParamLoader(Dataset):\n",
    "    def __init__(self, noisy_signals, reg_path, gcv_path, snrs, c1_t, gcv: bool=False):\n",
    "        print('BEGIN MULTIPLE DECAYS!!!')\n",
    "        times = torch.linspace(8.0, 256.0, 32)\n",
    "        df = pd.read_feather(reg_path)\n",
    "        df_gcv = pd.read_feather(gcv_path)\n",
    "        if gcv:\n",
    "            df_gcv[\"Index\"] = df_gcv[\"Index\"].astype(int)\n",
    "            df_gcv.set_index(\"Index\", inplace = True, drop = True)\n",
    "            df_gcv.sort_index(inplace = True)\n",
    "        regTraj_noisySignals = torch.from_numpy(np.stack(df[\"ND\"].values))\n",
    "        gcv_noisySignals = torch.from_numpy(np.stack(df_gcv[\"ND\"].values))\n",
    "        ND_tens = torch.from_numpy(noisy_signals)\n",
    "        bool = torch.equal(regTraj_noisySignals.float(), ND_tens.float())\n",
    "        print(\"IDX BASED? \", bool)\n",
    "        assert(bool)\n",
    "        bool = torch.equal(gcv_noisySignals.float(), ND_tens.float())\n",
    "        print(\"IDX BASED? \", bool)\n",
    "        assert(bool)\n",
    "        T2_values = torch.tensor(df[[\"t21_ld\", \"t22_ld\"]].values)\n",
    "        c1_ld = torch.tensor(df[\"c1_ld\"].values)\n",
    "        recon_signals = self._signalModel(times.reshape(1,-1), c1_ld.reshape(-1,1), T2_values[:,0].reshape(-1,1), T2_values[:,1].reshape(-1,1))\n",
    "        stacked = torch.stack([ND_tens, recon_signals], dim =1)\n",
    "        self.training_tensor_proc = stacked.flatten(1).float()\n",
    "        gcv_T2_values = torch.tensor(df_gcv[[\"t21_ld\", \"t22_ld\"]].values)\n",
    "        gcv_c1_ld = torch.tensor(df_gcv[\"c1_ld\"].values)\n",
    "        recon_signals_gcv = self._signalModel(times.reshape(1,-1), gcv_c1_ld.reshape(-1,1), gcv_T2_values[:,0].reshape(-1,1), gcv_T2_values[:,1].reshape(-1,1))\n",
    "        stacked_gcv = torch.stack([ND_tens, recon_signals_gcv], dim =1)\n",
    "        self.gcv_training_tensor_proc = stacked_gcv.flatten(1).float()\n",
    "        self.target_tensor_proc = torch.from_numpy(c1_t).float()\n",
    "        self.snrs = snrs\n",
    "\n",
    "    def __len__(self):\n",
    "        #decay_input = ['ND', 'NB']\n",
    "        \"\"\"\n",
    "        Returns the number of samples in the data set\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        return self.training_tensor_proc.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        Returns samples at idx.\n",
    "\n",
    "        - If idx is an int, the output is a pair of 1-d Tensors: the first is\n",
    "\n",
    "        the regularized decays stacked horizontally and the second is the\n",
    "\n",
    "        corresponding target parameters.\n",
    "\n",
    "        - If idx is a list, it returns two Tensors, each with 2 dimensions.\n",
    "\n",
    "        The first index points to the particular sample and the second index\n",
    "\n",
    "        points to an intensity at some time for some regularized decay.\n",
    "\n",
    "        \"\"\"\n",
    "        #decay_input = ['ND', 'NB']\n",
    "        ND_tensor = self.training_tensor_proc[idx]\n",
    "        target_params = self.target_tensor_proc[idx]\n",
    "        # target = self.target_tensor_proc[idx]\n",
    "\n",
    "        # print(multiple_decays)\n",
    "        return ND_tensor, self.gcv_training_tensor_proc[idx], target_params, self.snrs[idx]\n",
    "    \n",
    "    def _signalModel(self, t, c1, t21, t22):\n",
    "        return c1*torch.exp(-t/t21) + (1.0 - c1)*torch.exp(-t/t22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ce0fe429-8fb8-4e58-847f-adf6933ec257",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_NLLS = \"Brain_ILR/BrainData/BrainData/BLSA_1742_04_MCIAD_m41/NLLS_3PE_with_offset_Rician.feather\"\n",
    "df = pd.read_feather(path_NLLS)\n",
    "df[\"Index\"] = df[\"Index\"].astype(int)\n",
    "df.set_index(\"Index\", inplace = True, drop = True)\n",
    "df.sort_index(inplace = True)\n",
    "nd_off_array = np.array(df[[\"ND\",\"Offset\"]])\n",
    "ND = nd_off_array[:,0]\n",
    "ND = np.stack(ND)\n",
    "ND = np.float64(ND)\n",
    "offsets = np.float64(nd_off_array[:,1])\n",
    "const_value = np.pi/2\n",
    "snr_pred = ND[:,0]/(offsets/np.sqrt(const_value))\n",
    "snr_pred = torch.from_numpy(snr_pred).float()\n",
    "brain_NLLS_SNRs = torch.empty((288,288,1))\n",
    "brain_NLLS_SNRs[raw_masking_idxs] = snr_pred.unsqueeze(1)\n",
    "brain_NLLS_SNRs[~raw_masking_idxs] = 0.0\n",
    "brain_NLLS_c1s = torch.empty((288,288,1))\n",
    "brain_NLLS_c1s[raw_masking_idxs] = torch.tensor(df[\"c1_NLLS\"]).float().unsqueeze(1)\n",
    "brain_NLLS_c1s[~raw_masking_idxs] = df[\"c1_NLLS\"].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67980d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model(snr, device):\n",
    "    NDND_NAME = f\"3P_RicianNoise__ILR__NDND__SNR_{snr}\"\n",
    "    NDReg_NAME = f\"3P_NDReg__ILR__NDReg__SNR_{snr}\"\n",
    "    NDReg_GCV_NAME = f\"3P_NDReg__GCV__NDReg__SNR_{snr}\"\n",
    "\n",
    "    NDND_path = f\"./Three_Parameters/NIH_NN_Biexponentials/MakeDataset/NN_3P/results/_snr{snr}/{NDND_NAME}L2Loss_TE32_B64_lr1e4.pth\"\n",
    "    checkpoint_NDND = torch.load(NDND_path, map_location=device)\n",
    "\n",
    "    NDReg_path = f\"./Three_Parameters/NIH_NN_Biexponentials/MakeDataset/NN_3P/results/_snr{snr}/{NDReg_NAME}L2Loss_TE32_B64_lr1e4.pth\"\n",
    "    checkpoint_NDReg = torch.load(NDReg_path, map_location=device)\n",
    "\n",
    "    NDReg_GVC_path = f\"./Three_Parameters/NIH_NN_Biexponentials/MakeDataset/NN_3P/results/_snr{snr}/{NDReg_GCV_NAME}L2Loss_TE32_B64_lr1e4.pth\"\n",
    "    checkpoint_GCV_NDReg = torch.load(NDReg_GVC_path, map_location=device)\n",
    "\n",
    "    input_size = 64\n",
    "    H1,H2,H3,H4 = 32,256,256,32\n",
    "    output_size = 3\n",
    "    \n",
    "    model_NDND = LambdaTraining_FC_NN(input_size, H1, H2, H3, H4, output_size)\n",
    "    model_NDND.load_state_dict(checkpoint_NDND['model_state_dict'])\n",
    "    model_NDND.to(device)\n",
    "    model_NDND.eval()\n",
    "\n",
    "    model_NDReg = LambdaTraining_FC_NN(input_size, H1, H2, H3, H4, output_size)\n",
    "    model_NDReg.load_state_dict(checkpoint_NDReg['model_state_dict'])\n",
    "    model_NDReg.to(device)\n",
    "    model_NDReg.eval()\n",
    "    \n",
    "    model_NDReg_GCV = LambdaTraining_FC_NN(input_size, H1, H2, H3, H4, output_size)\n",
    "    model_NDReg_GCV.load_state_dict(checkpoint_GCV_NDReg['model_state_dict'])\n",
    "    model_NDReg_GCV.to(device)\n",
    "    model_NDReg_GCV.eval()\n",
    "    \n",
    "    return model_NDND, model_NDReg, model_NDReg_GCV\n",
    "\n",
    "def fetch_prediction(model_NDND, model_NDReg, model_NDReg_GCV, data, gcv_data):\n",
    "    \n",
    "\n",
    "    with torch.no_grad():\n",
    "        return model_NDND(data[:,:32].repeat(1,2)), model_NDReg(data), model_NDReg_GCV(gcv_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d63320f8-69fb-437b-8dac-5b79b0fa3328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEGIN MULTIPLE DECAYS!!!\n",
      "IDX BASED?  True\n",
      "IDX BASED?  True\n"
     ]
    }
   ],
   "source": [
    "gcv_path = \"/Brain_ILR/BrainData/BrainData/BLSA_1742_04_MCIAD_m41/GCV_Lambdas_BrainRicianNoise.feather\"\n",
    "reg_path = \"/brainRegSignals_RicianNoise_BinnedNNs.feather\" \n",
    "c1_ts = slice_c1[raw_masking_idxs]\n",
    "paramData = ParamLoader(raw_norm, os.getcwd() + reg_path, os.getcwd() + gcv_path, brain_NLLS_SNRs[raw_masking_idxs], c1_ts, gcv=True)\n",
    "paramLoader = DataLoader(paramData, batch_size = 1, shuffle=False, pin_memory=True, num_workers=64, persistent_workers=True) # Modify num_workers to desired number of processors  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6c3652",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    # display(training_preds_t_Regs.shape)\n",
    "    curr_pos = 0\n",
    "    end_pos = 0\n",
    "    # training_preds_t_Final_Diff = torch.empty(paramLoader.dataset.training_tensor_proc.shape[0], 1)\n",
    "    training_preds_t_Final_Diff_gcv = torch.empty(paramLoader.dataset.training_tensor_proc.shape[0], 1)\n",
    "    training_preds_t_Final_Diff_reg = torch.empty(paramLoader.dataset.training_tensor_proc.shape[0], 1)\n",
    "    training_preds_t_Final_Diff_regvsgcv = torch.empty(paramLoader.dataset.training_tensor_proc.shape[0], 1)\n",
    "    \n",
    "    \n",
    "    diff_NLLS = brain_NLLS_c1s - reshaped_c1\n",
    "    diff_NLLS[~raw_masking_idxs] = False\n",
    "    \n",
    "    diff_gcv = torch.empty(paramLoader.dataset.training_tensor_proc.shape[0], 1)\n",
    "    diff_reg = torch.empty(paramLoader.dataset.training_tensor_proc.shape[0], 1)\n",
    "    diff_ndnd = torch.empty(paramLoader.dataset.training_tensor_proc.shape[0], 1)\n",
    "\n",
    "    lis = []\n",
    "    \n",
    "    models_list = [init_model(snr, device) for snr in [5.0,50.0,100.0]]\n",
    "    \n",
    "    for idx, (noisy_decay, gcv_prediction, c1_t, snr) in enumerate(tqdm(paramLoader, unit = \"batch\")):\n",
    "        batch_size = noisy_decay.size(0)\n",
    "        end_pos += batch_size\n",
    "        # assert (torch.all(noisy_decay == (brainLoader.dataset.training_tensor_proc)[curr_pos : end_pos]))\n",
    "        noisy_decay, c1_t, gcv_prediction = noisy_decay.to(device), c1_t.to(device), gcv_prediction.to(device)\n",
    "        if snr < 25:\n",
    "            predictions_NDND, predictions_NDReg, predictions_gcv = fetch_prediction(*models_list[0], noisy_decay, gcv_prediction)\n",
    "\n",
    "        elif snr < 75:\n",
    "            predictions_NDND, predictions_NDReg, predictions_gcv = fetch_prediction(*models_list[1], noisy_decay, gcv_prediction)\n",
    "\n",
    "        else:\n",
    "            predictions_NDND, predictions_NDReg, predictions_gcv = fetch_prediction(*models_list[2], noisy_decay, gcv_prediction)\n",
    "\n",
    "\n",
    "        diff_gcv[curr_pos:end_pos] = predictions_gcv[:,2] - c1_t\n",
    "        diff_reg[curr_pos:end_pos] = predictions_NDReg[:,2] - c1_t\n",
    "        diff_ndnd[curr_pos:end_pos] = predictions_NDND[:,2] - c1_t\n",
    "\n",
    "        abs_diff_gcv = torch.abs(predictions_NDND[:,2]-c1_t) - torch.abs(predictions_gcv[:,2] - c1_t)\n",
    "        training_preds_t_Final_Diff_gcv[curr_pos : end_pos] = abs_diff_gcv\n",
    "\n",
    "        abs_diff_reg = torch.abs(predictions_NDND[:,2]-c1_t) - torch.abs(predictions_NDReg[:,2] - c1_t)\n",
    "        training_preds_t_Final_Diff_reg[curr_pos : end_pos] = abs_diff_reg\n",
    "\n",
    "        training_preds_t_Final_Diff_regvsgcv[curr_pos : end_pos] = torch.abs(predictions_gcv[:,2]-c1_t) - torch.abs(predictions_NDReg[:,2] - c1_t)\n",
    "\n",
    "        curr_pos += batch_size\n",
    "    \n",
    "    c1_error_gcv = torch.empty_like(torch.from_numpy(reshaped_c1))\n",
    "    c1_error_gcv[raw_masking_idxs] = training_preds_t_Final_Diff_gcv\n",
    "    c1_error_gcv[~raw_masking_idxs_2] = False\n",
    "\n",
    "    c1_error_reg = torch.empty_like(torch.from_numpy(reshaped_c1))\n",
    "    c1_error_reg[raw_masking_idxs] = training_preds_t_Final_Diff_reg\n",
    "    c1_error_reg[~raw_masking_idxs_2] = False\n",
    "\n",
    "    c1_error_regvsgcv = torch.empty_like(torch.from_numpy(reshaped_c1))\n",
    "    c1_error_regvsgcv[raw_masking_idxs] = training_preds_t_Final_Diff_regvsgcv\n",
    "    c1_error_regvsgcv[~raw_masking_idxs_2] = False\n",
    "    \n",
    "    \n",
    "    c1_error_diff_gcv = torch.empty_like(torch.from_numpy(reshaped_c1))\n",
    "    c1_error_diff_gcv[raw_masking_idxs] = diff_gcv\n",
    "    c1_error_diff_gcv[~raw_masking_idxs_2] = False\n",
    "    \n",
    "    c1_error_diff_reg = torch.empty_like(torch.from_numpy(reshaped_c1))\n",
    "    c1_error_diff_reg[raw_masking_idxs] = diff_reg\n",
    "    c1_error_diff_reg[~raw_masking_idxs_2] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b8ede6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,12), dpi = 300)\n",
    "plt.subplots_adjust(wspace = 0.15)\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "\n",
    "c1_min_error_bound = -0.50\n",
    "c1_max_error_bound = -1*c1_min_error_bound\n",
    "c1_ticks = np.arange(c1_min_error_bound,c1_max_error_bound+0.01, 0.1)\n",
    "\n",
    "\n",
    "c1_error_plot = fig.add_subplot(1,1,1)\n",
    "\n",
    "mask_bic = aic_mask.squeeze() \n",
    "x_bic = c1_error_gcv\n",
    "masked_bic_x = np.ma.array(x_bic, mask=mask_bic)\n",
    "\n",
    "c1_error_plot.imshow(mask, cmap=ListedColormap(['black']))\n",
    "im_c1_error = c1_error_plot.imshow(masked_bic_x[75:215, 56:227], cmap='seismic_r', vmin = c1_min_error_bound, vmax = c1_max_error_bound)\n",
    "c1_error_plot.set_title(\"MWF Relative Error\\n(ND, ND) versus (ND, Reg)$_{GCV}$\", fontsize = 35)\n",
    "fig.colorbar(im_c1_error, ax = c1_error_plot, location = \"bottom\",  ticks = c1_ticks, shrink = 0.6, pad = 0.05).set_label(\"(ND, ND) Superior                                       (ND, Reg)$_{GCV}$ Superior\", labelpad = 15, size = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc174930",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,12), dpi = 300)\n",
    "plt.subplots_adjust(wspace = 0.15)\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "\n",
    "c1_min_error_bound = -0.50\n",
    "c1_max_error_bound = -1*c1_min_error_bound\n",
    "c1_ticks = np.arange(c1_min_error_bound,c1_max_error_bound+0.01, 0.1)\n",
    "c1_error_plot = fig.add_subplot(1,1,1)\n",
    "mask_bic = aic_mask.squeeze()\n",
    "x_bic = c1_error_regvsgcv \n",
    "masked_bic_x = np.ma.array(x_bic, mask=mask_bic)\n",
    "c1_error_plot.imshow(mask, cmap=ListedColormap(['black']))\n",
    "im_c1_error = c1_error_plot.imshow(-1*masked_bic_x[75:215, 56:227], cmap='seismic_r', vmin = c1_min_error_bound, vmax = c1_max_error_bound)\n",
    "c1_error_plot.set_title(\"MWF Relative Error\\n(ND, Reg)$_{NN}$ versus (ND, Reg)$_{GCV}$\", fontsize = 35)\n",
    "fig.colorbar(im_c1_error, ax = c1_error_plot, location = \"bottom\",  ticks = c1_ticks, shrink = 0.6, pad = 0.05).set_label(\"(ND, Reg)$_{NN}$ Superior                                (ND, Reg)$_{GCV}$ Superior\", labelpad = 15, size = 20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3PE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
